# -*- coding: utf-8 -*-
"""Topo_FabioCNN_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YkLC1qsACi2Q5YlLZ2JwW8WzCwo4ylAK
"""

import os
from google.colab import drive
import tensorflow as tf
from tensorflow import keras
from keras.layers import Input, Dense, Conv2D, Flatten, MaxPool2D
from keras.models import Model

import scipy.io as sio
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.regularizers import l2
from keras.layers import Dropout

from keras.optimizers import gradient_descent_v2

MOUNTPOINT = "/content/gdrive"
DATADIR = os.path.join(MOUNTPOINT, "MyDrive")
drive.mount(MOUNTPOINT)

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

#Import dataset cv
#datasetEEG_extended_topo = sio.loadmat("/content/gdrive/MyDrive/Dataset_MapasTopo_EEG/DataSetCV/datasetEEG_extended_topo_media1s.mat")
#dado_csv = pd.read_csv('/content/gdrive/MyDrive/Dataset_MapasTopo_EEG/DataSetCV/datasetEEG_label_extended_topo_media1s.csv',header=None)

#Import dataset erpimage
#datasetEEG_extended_topo = sio.loadmat("/content/gdrive/MyDrive/Dataset_MapasTopo_EEG/DataSetCV/datasetEEG_extended_topo_erpimage500ms.mat")
#dado_csv = pd.read_csv('/content/gdrive/MyDrive/Dataset_MapasTopo_EEG/DataSetCV/datasetEEG_label_extended_topo_erpimage500ms.csv',header=None)

#Import dataset maxmin
datasetEEG_extended_topo = sio.loadmat("/content/gdrive/MyDrive/Dataset_MapasTopo_EEG/DataSetCV/datasetEEG_extended_topo_maxmin500ms.mat")
dado_csv = pd.read_csv('/content/gdrive/MyDrive/Dataset_MapasTopo_EEG/DataSetCV/datasetEEG_label_extended_topo_maxmin500ms.csv',header=None)

#prepare train and test sets
dado = datasetEEG_extended_topo["dataset2"]
datasetY = dado_csv.values

datasetX = np.stack(dado[:,0])
print(datasetX.shape)

imgplot = plt.imshow(datasetX[0])
plt.show() 

del dado
del dado_csv

#normaliza pixel values
datasetY = datasetY.astype('float32')
datasetX = datasetX.astype('float32')/ 255
#datasetY = datasetY.astype('float32')/ 1
#datasetX = datasetX.astype('float32')/ 1

X_train, X_test, y_train, y_test = train_test_split(datasetX, datasetY, test_size=0.33)

entrada = datasetX[0].shape
print(entrada)

del datasetX
del datasetY

from keras.optimizer_v2.gradient_descent import SGD
#CNN

# define model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(entrada)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# compile model
#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

opt = SGD(learning_rate=0.001, momentum=0.9)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])


# fit model
#model.fit(X_train, y_train, epochs=150, batch_size=128, verbose=0)
#model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)

# evaluate the model
#loss, acc = model.evaluate(X_test, y_test, verbose=1)
#print('Test Accuracy: %.3f' % acc)

#CNN 2
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(entrada)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.3))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
# compile model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 10:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

# fit model
#model.fit(X_train, y_train, epochs=150, batch_size=64, verbose=1)
history = model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1)

# evaluate the model
loss, acc = model.evaluate(X_test, y_test, verbose=1)
print('Test Accuracy: %.3f' % acc)

# plot loss
	plt.subplot(211)
	plt.title('Cross Entropy Loss')
	plt.plot(history.history['loss'], color='blue', label='ERP')
#	plt.plot(history.history['val_loss'], color='orange', label='test')
	# plot accuracy
	plt.subplot(212)
	plt.title('Classification Accuracy')
	plt.plot(history.history['accuracy'], color='blue', label='ERP')
#	plt.plot(history.history['val_accuracy'], color='orange', label='test')
	# save plot to file
#	filename = sys.argv[0].split('/')[-1]
#	plt.savefig(filename + '_plot.png')
#	plt.close()
